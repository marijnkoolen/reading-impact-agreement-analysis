Interrater agreement

To see if raters interpret the rating task in the same way, we computed Interrater Agreement (IRA). Following the review of IRA measures by O’Neill (2017), we use the IRA statistic $r^*_{wg}$ (Lindell and Brandt 1997) which is a modified version of the original r_{wg} statistic (James et al. 1984, James et al. 1993). The original was developed for rater reliability on Likert scales for single items, but was found to be more an indicator of agreement (James et al. 1993). One of the main drawbacks of the standard r_{wg} statistic is that it suffers from potentially inadmissible values. It computes the ratio of observed rater variance over the expected variance if raters provided random ratings, but can lead to agreement scores above + 1.0 or below 0. The modified r^*_{wg} takes values in the [-1,1] interval and statistic makes negative agreement scores are interpreteda ble (as a form of disagreement between subgroups of raters) and results in agreement scores that are always in the interval of admissible and interpretable values, e.g [-1,1]. Although there some concerns that Likert scale ratings cannot be interpreted as interval data (Jamieson 2004), others have pointed out that in many cases the errors produced by treating them as such are minimal (Norman 2010). 

The statistic is computed as:

	rwg = 1 -Sx22 

Where SX2 is the variance of the ratings for a sentence and σ2 is the expected variance based on a chosen theoretical null-distribution that represents a total lack of agreement. LeBreton and Senter (2008) argue that the choice of null-distribution should be guided by the specifics of the experiment and the biases in responses. The most used null-distribution is the uniform null (σeu2), which assumes that all ratings are equally likely to be chosen. For a 5-point Likert scale, that results in an expected variance of 2, but the actual variance of the raters can range between 0 and 4, hence the IRA can range between -1 and +1. Other choices of null-distribution make different assumptions of how likely different ratings are chosen. In some cases, raters may have a tendency to avoid the extreme ratings (central tendency), so the middle ratings are more likely, and the null-distribution should reflect this (e.g. a triangular distribution where the middle rating is twice as likely as its neighbouring ratings and three times as likely as the extreme ratings, see LeBreton and Senter (2008, p. 832)). Another option proposed by Lindell et al (1999) is using the maximum dissensus null-distribution  (σmv2), in which the expected variance is the maximum possible variance (raters always choose one of the extremes). As shown in Figure x, tThe 3896 ratings in our show a tendency towards the extremes., with the lowest possible rating being the most likely (39%), followed by the highest possible rating (20%), with the second lowest rating being the least likely (11%). This corresponds to an average expected variance of 2.54. This is somewhere in between the uniform null-distribution and maximum dissensus, and closest to On this basis we decided to use an 'inverse triangular' distribution with which has an expected variance of 2.55. We therefore adopt the inverse triangular null distribution as our theoretical null in calculating agreement.

Raters could indicate if they could not judge a sentence at all. This happened mostly for very short sentences that are uninterpretable without the rest of the review as context, such as ‘Uren.’ (‘Hours.’) or ‘En daar is eigenlijk maar een reden voor.’ (‘And there is only one reason for that’). Of the 348 sentences, 15 were marked as such by at least two of the raters, so no agreement can be computed. These are left out of the rest of the analysis. Although we could have screened the random selection of sentences to filter out such cases, this would make the sample less random and would introduce our own biases for what is interpretable.


Figure x. Distribution of IRA scores per sentence, for the four impact categories.

The distribution of IRA scores per sentence and impact category is shown in Figure x. The average IRA is moderate for emotional impact (0.62) and for aesthetic feeling (0.65), and weak for narrative feeling (0.49) and reflection (0.48). We see two main There are multiple possible explanations for this lower agreement for the latter two. One explanation is that raters interpreted these categories of impact differently resulting in different ratings. Another is that they did not have a concrete enough idea of what each category meant so that they rated inconsistently. It is also possible that the individual sentences did not give enough context for to clearly express the reviewer’s thinkingthought, forcing such that the raters to filled in the gaps with their own interpretations, and differed with each other in that respect. This last explanation is somewhat supported by the fact that raters indicated for some sentences that they could not judge them on these categories. It is plausible that sentences fall on a scale of clarity for being interpretable on their own, with sentences in the middle or low end of this scale giving enough signal of reading impact on the review author, such that raters felt they should score this, but that they were not confident in their answers. As a consequence, for those sentences, the Likert scale could at least partially reflect the rater’s confidence instead of just the clarity of the expression of reading impact. In this case, it would make sense to include a separate question for raters to indicate their own confidence in the scores they give for individual sentences. It would have been possible to ask the raters separately to rate their confidence for each answer, but we argue that this would have further complicated an already complex set of questions, and not necessarily have clarified how raters interpreted each category. 


